{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Azure OpenAI GPT-4 Vision to extract structured JSON data from PDF documents\n",
    "\n",
    "This notebook demonstrates [how to use GPT-4 Vision](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/gpt-with-vision?tabs=rest) to extract structured JSON data from PDF documents, such as invoices, using the [Azure OpenAI Service](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview).\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "The notebook uses [PowerShell](https://learn.microsoft.com/powershell/scripting/install/installing-powershell) and [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli) to deploy all necessary Azure resources. Both tools are available on Windows, macOS and Linux environments. It also uses [.NET 8](https://dotnet.microsoft.com/download/dotnet/8.0) to run the C# code that interacts with the Azure OpenAI Service.\n",
    "\n",
    "Running this notebook will deploy the following resources in your Azure subscription:\n",
    "- Azure Resource Group\n",
    "- Azure OpenAI Service (West US)\n",
    "- GPT-4 Vision model deployment (5K capacity)\n",
    "\n",
    "**Note**: The GPT-4 Vision model is currently in preview and is available in limited capacity (10K per region) in selected regions only. For more information, see the [Azure OpenAI Service documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-4-and-gpt-4-turbo-preview-model-availability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy infrastructure with Az CLI & Bicep\n",
    "\n",
    "The following will prompt you to login to Azure. Once logged in, the current default subscription in your available subscriptions will be set for deployment.\n",
    "\n",
    "> **Note:** If you have multiple subscriptions, you can change the default subscription by running `az account set --subscription <subscription_id>`.\n",
    "\n",
    "Then, all the necessary Azure resources will be deployed, previously listed, using [Azure Bicep](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/).\n",
    "\n",
    "The deployment occurs at the subscription level, creating a new resource group. The location of the deployment is set to **West US** and this can be changed to another location that supports the GPT-4 Vision model, as well as other parameters, in the [`./infra/main.bicepparam`](./infra/main.bicepparam) file.\n",
    "\n",
    "Once deployed, the Azure OpenAI Service endpoint and key will be stored in the [`./config.env`](./config.env) file for use in the .NET code.\n",
    "\n",
    "### Understanding the deployment\n",
    "\n",
    "#### OpenAI Services\n",
    "\n",
    "An [Azure OpenAI Service](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview) instance is deployed in the West US region. This is deployed with the `gpt-4-vision-preview` model to be used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "pwsh"
    },
    "polyglot_notebook": {
     "kernelName": "pwsh"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# Login to Azure\n",
    "Write-Host \"Checking if logged in to Azure...\"\n",
    "\n",
    "$loggedIn = az account show --query \"name\" -o tsv\n",
    "\n",
    "if ($loggedIn -ne $null) {\n",
    "    Write-Host \"Already logged in as $loggedIn\"\n",
    "    az login --tenant <YOUR_TENANT_ID>\n",
    "    az account set --subscription <YOUR_SUBSCRIPTION_ID>\n",
    "} else {\n",
    "    Write-Host \"Logging in...\"\n",
    "    az login --tenant <YOUR_TENANT_ID>\n",
    "    az account set --subscription <YOUR_SUBSCRIPTION_ID>\n",
    "}\n",
    "\n",
    "# Retrieve the default subscription ID\n",
    "$subscriptionId = (\n",
    "    (\n",
    "        az account list -o json `\n",
    "            --query \"[?isDefault]\"\n",
    "    ) | ConvertFrom-Json\n",
    ").id\n",
    "\n",
    "# Set the subscription\n",
    "az account set --subscription $subscriptionId\n",
    "Write-Host \"Subscription set to $subscriptionId\"\n",
    "\n",
    "# Deploy the infra/main.bicep file\n",
    "Write-Host \"Deploying the Bicep template...\"\n",
    "\n",
    "$deploymentOutputs = (az deployment sub create --name 'gpt-document-extraction' --location westus --template-file ./infra/main.bicep --parameters ./infra/main.bicepparam --query \"properties.outputs\" -o json) | ConvertFrom-Json\n",
    "\n",
    "# Get the Azure OpenAI Service API key\n",
    "$resourceGroupName = $deploymentOutputs.resourceGroupInfo.value.name\n",
    "$openAIName = $deploymentOutputs.openAIInfo.value.name\n",
    "$openAIEndpoint = $deploymentOutputs.openAIInfo.value.endpoint\n",
    "$openAIVisionModelDeploymentName = $deploymentOutputs.openAIInfo.value.visionModelDeploymentName\n",
    "$openAIKey = (az cognitiveservices account keys list --name $openAIName --resource-group $resourceGroupName --query key1 -o tsv)\n",
    "\n",
    "# Save the deployment outputs to a .env file\n",
    "Write-Host \"Saving the deployment outputs to a config.env file...\"\n",
    "\n",
    "function Set-ConfigurationFileVariable($configurationFile, $variableName, $variableValue) {\n",
    "    if (Select-String -Path $configurationFile -Pattern $variableName) {\n",
    "        (Get-Content $configurationFile) | Foreach-Object {\n",
    "            $_ -replace \"$variableName = .*\", \"$variableName = $variableValue\"\n",
    "        } | Set-Content $configurationFile\n",
    "    } else {\n",
    "        Add-Content -Path $configurationFile -value \"$variableName = $variableValue\"\n",
    "    }\n",
    "}\n",
    "\n",
    "$configurationFile = \"config.env\"\n",
    "\n",
    "if (-not (Test-Path $configurationFile)) {\n",
    "    New-Item -Path $configurationFile -ItemType \"file\" -Value \"\"\n",
    "}\n",
    "\n",
    "Set-ConfigurationFileVariable $configurationFile \"AZURE_RESOURCE_GROUP_NAME\" $resourceGroupName\n",
    "Set-ConfigurationFileVariable $configurationFile \"AZURE_OPENAI_ENDPOINT\" $openAIEndpoint\n",
    "Set-ConfigurationFileVariable $configurationFile \"AZURE_OPENAI_API_KEY\" $openAIKey\n",
    "Set-ConfigurationFileVariable $configurationFile \"AZURE_OPENAI_VISION_MODEL_DEPLOYMENT_NAME\" $openAIVisionModelDeploymentName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install .NET dependencies\n",
    "\n",
    "This notebook uses .NET to interact with the Azure OpenAI Service. It takes advantage of the following NuGet packages:\n",
    "\n",
    "### PDFtoImage\n",
    "\n",
    "The [PDFtoImage](https://github.com/sungaila/PDFtoImage) library is used to convert PDF documents to JPEG images. The library provides a simple layer to convert PDF documents using the static `PDFtoImage.Conversion` class. Reading the bytes of the PDF, the library will create an image and store it with a given file name.\n",
    "\n",
    "### DotNetEnv\n",
    "\n",
    "The [DotNetEnv](https://github.com/tonerdo/dotnet-env) library is used to load environment variables from a `.env` file which can be accessed via the `Environment.GetEnvironmentVariable(string)` method. This library is used to load the Azure OpenAI Service endpoint, key and model deployment name from the [`./config.env`](./config.env) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget:System.Text.Json, 8.0.1\"\n",
    "#r \"nuget:DotNetEnv, 3.0.0\"\n",
    "#r \"nuget:PDFtoImage, 4.0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "using System.Net;\n",
    "using System.Net.Http;\n",
    "using System.Text.Json.Nodes;\n",
    "using System.Text.Json;\n",
    "using System.IO; \n",
    "\n",
    "using DotNetEnv;\n",
    "using PDFtoImage;\n",
    "using SkiaSharp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "Env.Load(\"config.env\");\n",
    "\n",
    "var endpoint = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_ENDPOINT\");\n",
    "var apiKey = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_API_KEY\");\n",
    "var modelDeployment = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_VISION_MODEL_DEPLOYMENT_NAME\");\n",
    "var apiVersion = \"2023-12-01-preview\";\n",
    "\n",
    "var pdfName = \"TR4-ESHS6500381 2017_10_17 01_45_45.pdf\";\n",
    "var pdfImageName = \"TR4-ESHS6500381 2017_10_17 01_45_45.pdf_stitched.jpg\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert PDF to image\n",
    "\n",
    "For the GPT-4 Vision model to extract structured JSON data from a PDF document, the document must first be converted to an image. The following code demonstrates how to convert a PDF document to a JPEG image using the `PDFtoImage` library.\n",
    "\n",
    "### Important notes for image analysis with the GPT-4 Vision model\n",
    "\n",
    "- The maximum size for images is restricted to 20MB.\n",
    "- The `image_url` parameter in the message body has a `detail` property that can be set to `low` to enable a lower resolution image analysis for faster results with fewer tokens. However, this could impact the accuracy of the result.\n",
    "- When providing images, there is a limit of 10 images per call.\n",
    "\n",
    "Based on these notes, you may need to perform pre-processing of your PDF when converting it to images to ensure that the images are within the size limits and that the resolution is appropriate for the analysis. This may include:\n",
    "\n",
    "- Reducing the resolution of the images.\n",
    "- Splitting the PDF into multiple images, if it contains less than 10 pages.\n",
    "- Stitching multiple images together, if the PDF contains more than 10 pages.\n",
    "- Compressing the images to reduce the file size.\n",
    "\n",
    "Experiment with different pre-processing techniques to find the best approach for your specific use case.\n",
    "\n",
    "The following code provides examples using .NET to convert a PDF document with multiple pages into one image that stitches the pages together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var pdf = await File.ReadAllBytesAsync(pdfName);\n",
    "var pageImages = PDFtoImage.Conversion.ToImages(pdf);\n",
    "\n",
    "int totalHeight = pageImages.Sum(image => image.Height);\n",
    "int width = pageImages.Max(image => image.Width);\n",
    "\n",
    "var stitchedImage = new SKBitmap(width, totalHeight);\n",
    "var canvas = new SKCanvas(stitchedImage);\n",
    "\n",
    "int currentHeight = 0;\n",
    "foreach (var pageImage in pageImages)\n",
    "{\n",
    "    canvas.DrawBitmap(pageImage, 0, currentHeight);\n",
    "    currentHeight += pageImage.Height;\n",
    "}\n",
    "\n",
    "using (var stitchedFileStream = new FileStream(pdfImageName, FileMode.Create, FileAccess.Write))\n",
    "{\n",
    "    stitchedImage.Encode(stitchedFileStream, SKEncodedImageFormat.Jpeg, 100);\n",
    "}\n",
    "\n",
    "Console.WriteLine($\"Stitched {pdfName} into {pdfName}_stitched.jpg\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GPT-4-Vision-Preview to extract the data from the image\n",
    "\n",
    "Now that the PDF document has been converted to an image, the GPT-4 Vision model can be used to extract structured JSON data from the image. The following code demonstrates how to use the deployed Azure OpenAI Service directly via the API to extract structured JSON data from the image.\n",
    "\n",
    "In this example, the payload for the Chat completion endpoint is a JSON object with the following details:\n",
    "\n",
    "### System Prompt\n",
    "\n",
    "The system prompt is the instruction to the model that prescribes the model's behavior. They allow you to constrain the model's behavior to a specific task, making it more adaptable for specific use cases, such as extracting structured JSON data from documents.\n",
    "\n",
    "In this case, it is to extract structured JSON data from the image. Here is what we have provided:\n",
    "\n",
    "**You are an AI assistant that extracts data from documents and returns them as structured JSON objects. If a value is not present, provide null. The TR4 document has multiple sections with location information, applicant information, plot diagram, and test report. Do not return as a code block. Use only information extracted from image. Do not make up any information.**\n",
    "\n",
    "> **Note:** GPT-4 Vision doesn't currently allow the `response_format` parameter to be set to `json`. To avoid the response being returned as a code block, we have included the instruction to not return as a code block. \n",
    "\n",
    "Learn more about [system prompts](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/system-message).\n",
    "\n",
    "### User Prompt\n",
    "\n",
    "The user prompt is the input to the model that provides context for the model's response. It is the input that the model uses to generate a response. \n",
    "\n",
    "In this case, it is the image of the document plus some additional text context to help the model understand the task. Here is what we have provided:\n",
    "\n",
    "**Extract the data from this Soil Investigation Technical Report. There may be multiple Boring pits and soil Test Pits in section 5 Test Report in which case an array can be created for each boring. For the Plot Diagram sections, provide a textual summary for the PlotDiagramSummaryDetails field according to the extracted measurements for an engineer to read. The plot diagram is for the address found in location information section of the document. The Plot diagram drawing will show boring points. Extract the distance for each boring if available in the image and place in the respective DistanceFromNorthBoundary, DistanceFromSouthBoundary, DistanceFromEastBoundary, or DistanceFromSouthBoundary fields. If no value then value should be null. Use the following structure, {\\\"DocumentInformation\\\", {\\\"DocumentType\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"JobNumber\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"ScanCode\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}}, \\\"LocationDetails\\\", {\\\"HouseNumber\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"StreetName\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"Borough\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"Block\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"Lot\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"BIN\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"CommunityBoardNumber\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"WorkOnFloors\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"ApartmentNumber\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}}, \\\"ApplicantInformation\\\", {\\\"LastName\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"FirstName\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"MiddleInitial\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"BusinessName\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"BusinessAddress\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"BusinessPhone\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"MobilePhone\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"Fax\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"State\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"City\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"Zip\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"Email\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"LicenseNumber\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"PEcheck\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"RAcheck\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}}, \\\"BoringDetails\\\", [{\\\"BoringNumber\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"PlotDiagramSummary\\\", \\\"PlotDiagramSummaryDetails\\\",{\\\"DistanceFromNorthBoundary\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"DistanceFromEastBoundary\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"DistanceFromSouthBoundary\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"DistanceFromWestBoundary\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}}, \\\"BoringDate\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"FeetBelowCurb\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"SoilDescription\\\", [{\\\"Depth\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"Description\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"ClassNumber\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}, \\\"Remarks\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}}]}], \\\"AdditionalRemarks\\\", {\\\"Remarks\\\", {\\\"value\\\", \\\"\\\", \\\"\\\"}}}\" }**\n",
    "\n",
    "> **Note:** For the user prompt, it is ideal to provide a structure for the JSON response. Without one, the model will determine this for you and you may not get consistency across responses. \n",
    "\n",
    "This prompt ensures that the model understands the task, and the additional text context provides the model with the necessary information to extract the structured JSON data from the image. This approach would result in a response similar to the following:\n",
    "\n",
    "```json\n",
    "{  \n",
    "  \"DocumentInformation\": {  \n",
    "    \"DocumentType\": \"TR4: Technical Report - Soil Investigation\",  \n",
    "    \"JobNumber\": \"321186970\",  \n",
    "    \"ScanCode\": \"ES397871022\"  \n",
    "  },  \n",
    "  \"LocationDetails\": {  \n",
    "    \"HouseNumber\": \"13\",  \n",
    "    \"StreetName\": \"Somers Street\",  \n",
    "    \"Borough\": \"Brooklyn\",  \n",
    "    \"Block\": \"1538\",  \n",
    "    \"Lot\": \"64\",  \n",
    "    \"BIN\": null,  \n",
    "    \"CommunityBoardNumber\": null,  \n",
    "    \"WorkOnFloors\": null,  \n",
    "    \"ApartmentNumber\": null  \n",
    "  },  \n",
    "  \"ApplicantInformation\": {  \n",
    "    \"LastName\": \"Lent\",  \n",
    "    \"FirstName\": \"Sanford\",  \n",
    "    \"MiddleInitial\": \"E\",  \n",
    "    \"BusinessName\": \"All Phase Testing, Inc.\",  \n",
    "    \"BusinessAddress\": \"3319 Merritt Avenue\",  \n",
    "    \"BusinessPhone\": \"(718) 994-3200\",  \n",
    "    \"MobilePhone\": null,  \n",
    "    \"Fax\": \"(718) 994-5406\",  \n",
    "    \"State\": \"NY\",  \n",
    "    \"City\": \"Bronx\",  \n",
    "    \"Zip\": \"10475\",  \n",
    "    \"Email\": \"info@allphasetesting.com\",  \n",
    "    \"LicenseNumber\": \"046732\",  \n",
    "    \"PEcheck\": true,  \n",
    "    \"RAcheck\": false  \n",
    "  },  \n",
    "  \"BoringDetails\": [  \n",
    "    {  \n",
    "      \"BoringNumber\": \"B1\",  \n",
    "      \"PlotDiagramSummary\": \"Boring B1 is located approximately 131' 50\\\" from Hopkinson Avenue and 5' 0\\\" from Somers Street.\",  \n",
    "      \"DistanceFromNorthBoundary\": null,  \n",
    "      \"DistanceFromEastBoundary\": null,  \n",
    "      \"DistanceFromSouthBoundary\": \"5' 0\\\"\",  \n",
    "      \"DistanceFromWestBoundary\": \"131' 50\\\"\",  \n",
    "      \"BoringDate\": null,  \n",
    "      \"FeetBelowCurb\": null,  \n",
    "      \"SoilDescription\": [],  \n",
    "      \"Remarks\": null  \n",
    "    },  \n",
    "    {  \n",
    "      \"BoringNumber\": \"B2\",  \n",
    "      \"PlotDiagramSummary\": \"Boring B2 is located approximately 23' 0\\\" from the north boundary and 55' 0\\\" from Boring B1.\",  \n",
    "      \"DistanceFromNorthBoundary\": \"23' 0\\\"\",  \n",
    "      \"DistanceFromEastBoundary\": null,  \n",
    "      \"DistanceFromSouthBoundary\": null,  \n",
    "      \"DistanceFromWestBoundary\": \"55' 0\\\"\",  \n",
    "      \"BoringDate\": null,  \n",
    "      \"FeetBelowCurb\": null,  \n",
    "      \"SoilDescription\": [],  \n",
    "      \"Remarks\": null  \n",
    "    }  \n",
    "  ],  \n",
    "  \"AdditionalRemarks\": {  \n",
    "    \"Remarks\": \"See boring report B-001.00 dated 1/22/17.\"  \n",
    "  }  \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var base64Image = Convert.ToBase64String(File.ReadAllBytes(pdfImageName));\n",
    "\n",
    "JsonObject jsonPayload = new JsonObject\n",
    "{\n",
    "    {\n",
    "        \"messages\", new JsonArray \n",
    "        {\n",
    "            new JsonObject\n",
    "            {\n",
    "                { \"role\", \"system\" },\n",
    "                { \"content\", \"You are an AI assistant that extracts data from documents and returns them as structured JSON objects. If a value is not present, provide null. The TR4 document has multiple sections with location information, applicant information, plot diagram, and test report. Do not return as a code block. Use only information extracted from image. Do not make up any information.\" }\n",
    "            },\n",
    "            new JsonObject\n",
    "            {\n",
    "                { \"role\", \"user\" },\n",
    "                { \"content\",\n",
    "                    new JsonArray\n",
    "                    {\n",
    "                        new JsonObject\n",
    "                        {\n",
    "                            { \"type\", \"text\" },\n",
    "                            { \"text\", \"Format the data from this Soil Investigation Technical Report document intelligence output. Use the following structure: {\\\"DocumentInformation\\\": {\\\"DocumentType\\\": \\\"\\\", \\\"JobNumber\\\": \\\"\\\", \\\"ScanCode\\\": \\\"\\\"}, \\\"LocationDetails\\\": {\\\"HouseNumber\\\": \\\"\\\", \\\"StreetName\\\": \\\"\\\", \\\"Borough\\\": \\\"\\\", \\\"Block\\\": \\\"\\\", \\\"Lot\\\": \\\"\\\", \\\"BIN\\\": \\\"\\\", \\\"CommunityBoardNumber\\\": \\\"\\\", \\\"WorkOnFloors\\\": \\\"\\\", \\\"ApartmentNumber\\\": \\\"\\\"}, \\\"ApplicantInformation\\\": {\\\"LastName\\\": \\\"\\\", \\\"FirstName\\\": \\\"\\\", \\\"MiddleInitial\\\": \\\"\\\", \\\"BusinessName\\\": \\\"\\\", \\\"BusinessAddress\\\": \\\"\\\", \\\"BusinessPhone\\\": \\\"\\\", \\\"MobilePhone\\\": \\\"\\\", \\\"Fax\\\": \\\"\\\", \\\"State\\\": \\\"\\\", \\\"City\\\": \\\"\\\", \\\"Zip\\\": \\\"\\\", \\\"Email\\\": \\\"\\\", \\\"LicenseNumber\\\": \\\"\\\", \\\"PEcheck\\\": \\\"\\\", \\\"RAcheck\\\": \\\"\\\"}, \\\"BoringDetails\\\": [{\\\"BoringNumber\\\": \\\"\\\", \\\"BoringDate\\\": \\\"\\\", \\\"BoringNumber\\\": \\\"\\\", \\\"FeetBelowCurb\\\": \\\"\\\", \\\"SoilDescription\\\": \\\"\\\", \\\"ClassNumber\\\": \\\"\\\"}], \\\"AdditionalRemarks\\\": {\\\"Remarks\\\": \\\"\\\"}}\" }\n",
    "                        },\n",
    "                        new JsonObject\n",
    "                        {\n",
    "                            { \"type\", \"image_url\" },\n",
    "                            { \"image_url\", new JsonObject { { \"url\", $\"data:image/jpeg;base64,{base64Image}\" } } }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    { \"model\", modelDeployment },\n",
    "    { \"max_tokens\", 4096 },\n",
    "    { \"temperature\", 0 },\n",
    "    { \"top_p\", 0 },\n",
    "};\n",
    "\n",
    "string payload = JsonSerializer.Serialize(jsonPayload, new JsonSerializerOptions\n",
    "{\n",
    "    WriteIndented = true\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "string visionEndpoint = $\"{endpoint}openai/deployments/{modelDeployment}/chat/completions?api-version={apiVersion}\";\n",
    "\n",
    "using (HttpClient httpClient = new HttpClient())\n",
    "{\n",
    "    httpClient.BaseAddress = new Uri(visionEndpoint);\n",
    "    httpClient.DefaultRequestHeaders.Add(\"api-key\", apiKey);\n",
    "    httpClient.DefaultRequestHeaders.Accept.Add(new System.Net.Http.Headers.MediaTypeWithQualityHeaderValue(\"application/json\"));\n",
    "\n",
    "    var stringContent = new StringContent(payload, Encoding.UTF8, \"application/json\");\n",
    "\n",
    "    var response = await httpClient.PostAsync(visionEndpoint, stringContent);\n",
    "\n",
    "    if (response.IsSuccessStatusCode)\n",
    "    {\n",
    "        using (var responseStream = await response.Content.ReadAsStreamAsync())\n",
    "        {\n",
    "            // Parse the JSON response using JsonDocument\n",
    "            using (var jsonDoc = await JsonDocument.ParseAsync(responseStream))\n",
    "            {\n",
    "                // Access the message content dynamically\n",
    "                JsonElement jsonElement = jsonDoc.RootElement;\n",
    "                string messageContent = jsonElement.GetProperty(\"choices\")[0].GetProperty(\"message\").GetProperty(\"content\").GetString();\n",
    "\n",
    "                // Output the message content\n",
    "                Console.WriteLine(messageContent);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        Console.WriteLine(response);\n",
    "    }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "python"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
