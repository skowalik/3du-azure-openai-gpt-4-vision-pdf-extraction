{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Azure OpenAI GPT-4 Vision to extract structured JSON data from PDF documents\n",
    "\n",
    "This notebook demonstrates [how to use GPT-4 Vision](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/gpt-with-vision?tabs=rest) to extract structured JSON data from PDF documents, such as invoices, using the [Azure OpenAI Service](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview).\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "The notebook uses [PowerShell](https://learn.microsoft.com/powershell/scripting/install/installing-powershell) and [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli) to deploy all necessary Azure resources. Both tools are available on Windows, macOS and Linux environments. It also uses [.NET 8](https://dotnet.microsoft.com/download/dotnet/8.0) to run the C# code that interacts with the Azure OpenAI Service.\n",
    "\n",
    "Running this notebook will deploy the following resources in your Azure subscription:\n",
    "- Azure Resource Group\n",
    "- Azure OpenAI Service (West US)\n",
    "- GPT-4 Vision model deployment (5K capacity)\n",
    "\n",
    "**Note**: The GPT-4 Vision model is currently in preview and is available in limited capacity (10K per region) in selected regions only. For more information, see the [Azure OpenAI Service documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-4-and-gpt-4-turbo-preview-model-availability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy infrastructure with Az CLI & Bicep\n",
    "\n",
    "The following will prompt you to login to Azure. Once logged in, the current default subscription in your available subscriptions will be set for deployment.\n",
    "\n",
    "> **Note:** If you have multiple subscriptions, you can change the default subscription by running `az account set --subscription <subscription_id>`.\n",
    "\n",
    "Then, all the necessary Azure resources will be deployed, previously listed, using [Azure Bicep](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/).\n",
    "\n",
    "The deployment occurs at the subscription level, creating a new resource group. The location of the deployment is set to **West US** and this can be changed to another location that supports the GPT-4 Vision model, as well as other parameters, in the [`./infra/main.bicepparam`](./infra/main.bicepparam) file.\n",
    "\n",
    "Once deployed, the Azure OpenAI Service endpoint and key will be stored in the [`./config.env`](./config.env) file for use in the .NET code.\n",
    "\n",
    "### Understanding the deployment\n",
    "\n",
    "#### OpenAI Services\n",
    "\n",
    "An [Azure OpenAI Service](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview) instance is deployed in the West US region. This is deployed with the `gpt-4-vision-preview` model to be used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "pwsh"
    },
    "polyglot_notebook": {
     "kernelName": "pwsh"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# Login to Azure\n",
    "Write-Host \"Checking if logged in to Azure...\"\n",
    "\n",
    "$loggedIn = az account show --query \"name\" -o tsv\n",
    "\n",
    "if ($loggedIn -ne $null) {\n",
    "    Write-Host \"Already logged in as $loggedIn\"\n",
    "    az login --tenant <YOUR_TENANT_ID>\n",
    "    az account set --subscription <YOUR_SUBSCRIPTION_ID>\n",
    "} else {\n",
    "    Write-Host \"Logging in...\"\n",
    "    az login --tenant <YOUR_TENANT_ID>\n",
    "    az account set --subscription <YOUR_SUBSCRIPTION_ID>\n",
    "}\n",
    "\n",
    "# Retrieve the default subscription ID\n",
    "$subscriptionId = (\n",
    "    (\n",
    "        az account list -o json `\n",
    "            --query \"[?isDefault]\"\n",
    "    ) | ConvertFrom-Json\n",
    ").id\n",
    "\n",
    "# Set the subscription\n",
    "az account set --subscription $subscriptionId\n",
    "Write-Host \"Subscription set to $subscriptionId\"\n",
    "\n",
    "# Deploy the infra/main.bicep file\n",
    "Write-Host \"Deploying the Bicep template...\"\n",
    "\n",
    "$deploymentOutputs = (az deployment sub create --name 'gpt-document-extraction' --location westus --template-file ./infra/main.bicep --parameters ./infra/main.bicepparam --query \"properties.outputs\" -o json) | ConvertFrom-Json\n",
    "\n",
    "# Get the Azure OpenAI Service API key\n",
    "$resourceGroupName = $deploymentOutputs.resourceGroupInfo.value.name\n",
    "$openAIName = $deploymentOutputs.openAIInfo.value.name\n",
    "$openAIEndpoint = $deploymentOutputs.openAIInfo.value.endpoint\n",
    "$openAIVisionModelDeploymentName = $deploymentOutputs.openAIInfo.value.visionModelDeploymentName\n",
    "$openAIKey = (az cognitiveservices account keys list --name $openAIName --resource-group $resourceGroupName --query key1 -o tsv)\n",
    "\n",
    "# Save the deployment outputs to a .env file\n",
    "Write-Host \"Saving the deployment outputs to a config.env file...\"\n",
    "\n",
    "function Set-ConfigurationFileVariable($configurationFile, $variableName, $variableValue) {\n",
    "    if (Select-String -Path $configurationFile -Pattern $variableName) {\n",
    "        (Get-Content $configurationFile) | Foreach-Object {\n",
    "            $_ -replace \"$variableName = .*\", \"$variableName = $variableValue\"\n",
    "        } | Set-Content $configurationFile\n",
    "    } else {\n",
    "        Add-Content -Path $configurationFile -value \"$variableName = $variableValue\"\n",
    "    }\n",
    "}\n",
    "\n",
    "$configurationFile = \"config.env\"\n",
    "\n",
    "if (-not (Test-Path $configurationFile)) {\n",
    "    New-Item -Path $configurationFile -ItemType \"file\" -Value \"\"\n",
    "}\n",
    "\n",
    "Set-ConfigurationFileVariable $configurationFile \"AZURE_RESOURCE_GROUP_NAME\" $resourceGroupName\n",
    "Set-ConfigurationFileVariable $configurationFile \"AZURE_OPENAI_ENDPOINT\" $openAIEndpoint\n",
    "Set-ConfigurationFileVariable $configurationFile \"AZURE_OPENAI_API_KEY\" $openAIKey\n",
    "Set-ConfigurationFileVariable $configurationFile \"AZURE_OPENAI_VISION_MODEL_DEPLOYMENT_NAME\" $openAIVisionModelDeploymentName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install .NET dependencies\n",
    "\n",
    "This notebook uses .NET to interact with the Azure OpenAI Service. It takes advantage of the following NuGet packages:\n",
    "\n",
    "### PDFtoImage\n",
    "\n",
    "The [PDFtoImage](https://github.com/sungaila/PDFtoImage) library is used to convert PDF documents to JPEG images. The library provides a simple layer to convert PDF documents using the static `PDFtoImage.Conversion` class. Reading the bytes of the PDF, the library will create an image and store it with a given file name.\n",
    "\n",
    "### DotNetEnv\n",
    "\n",
    "The [DotNetEnv](https://github.com/tonerdo/dotnet-env) library is used to load environment variables from a `.env` file which can be accessed via the `Environment.GetEnvironmentVariable(string)` method. This library is used to load the Azure OpenAI Service endpoint, key and model deployment name from the [`./config.env`](./config.env) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget:System.Text.Json, 8.0.1\"\n",
    "#r \"nuget:DotNetEnv, 3.0.0\"\n",
    "#r \"nuget:PDFtoImage, 4.0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "using System.Net;\n",
    "using System.Net.Http;\n",
    "using System.Text.Json.Nodes;\n",
    "using System.Text.Json;\n",
    "using System.IO; \n",
    "\n",
    "using DotNetEnv;\n",
    "using PDFtoImage;\n",
    "using SkiaSharp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "Env.Load(\"config.env\");\n",
    "\n",
    "var endpoint = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_ENDPOINT\");\n",
    "var apiKey = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_API_KEY\");\n",
    "var modelDeployment = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_VISION_MODEL_DEPLOYMENT_NAME\");\n",
    "var apiVersion = \"2023-12-01-preview\";\n",
    "\n",
    "var pdfName = \"TR4-ES397871022 2017_10_16 10_20_15.pdf\";\n",
    "var pdfImageName = \"TR4-ES397871022 2017_10_16 10_20_15.pdf_stitched.jpg\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert PDF to image\n",
    "\n",
    "For the GPT-4 Vision model to extract structured JSON data from a PDF document, the document must first be converted to an image. The following code demonstrates how to convert a PDF document to a JPEG image using the `PDFtoImage` library.\n",
    "\n",
    "### Important notes for image analysis with the GPT-4 Vision model\n",
    "\n",
    "- The maximum size for images is restricted to 20MB.\n",
    "- The `image_url` parameter in the message body has a `detail` property that can be set to `low` to enable a lower resolution image analysis for faster results with fewer tokens. However, this could impact the accuracy of the result.\n",
    "- When providing images, there is a limit of 10 images per call.\n",
    "\n",
    "Based on these notes, you may need to perform pre-processing of your PDF when converting it to images to ensure that the images are within the size limits and that the resolution is appropriate for the analysis. This may include:\n",
    "\n",
    "- Reducing the resolution of the images.\n",
    "- Splitting the PDF into multiple images, if it contains less than 10 pages.\n",
    "- Stitching multiple images together, if the PDF contains more than 10 pages.\n",
    "- Compressing the images to reduce the file size.\n",
    "\n",
    "Experiment with different pre-processing techniques to find the best approach for your specific use case.\n",
    "\n",
    "The following code provides examples using .NET to convert a PDF document with multiple pages into one image that stitches the pages together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var pdf = await File.ReadAllBytesAsync(pdfName);\n",
    "var pageImages = PDFtoImage.Conversion.ToImages(pdf);\n",
    "\n",
    "int totalHeight = pageImages.Sum(image => image.Height);\n",
    "int width = pageImages.Max(image => image.Width);\n",
    "\n",
    "var stitchedImage = new SKBitmap(width, totalHeight);\n",
    "var canvas = new SKCanvas(stitchedImage);\n",
    "\n",
    "int currentHeight = 0;\n",
    "foreach (var pageImage in pageImages)\n",
    "{\n",
    "    canvas.DrawBitmap(pageImage, 0, currentHeight);\n",
    "    currentHeight += pageImage.Height;\n",
    "}\n",
    "\n",
    "using (var stitchedFileStream = new FileStream(pdfImageName, FileMode.Create, FileAccess.Write))\n",
    "{\n",
    "    stitchedImage.Encode(stitchedFileStream, SKEncodedImageFormat.Jpeg, 100);\n",
    "}\n",
    "\n",
    "Console.WriteLine($\"Stitched {pdfName} into {pdfName}_stitched.jpg\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GPT-4-Vision-Preview to extract the data from the image\n",
    "\n",
    "Now that the PDF document has been converted to an image, the GPT-4 Vision model can be used to extract structured JSON data from the image. The following code demonstrates how to use the deployed Azure OpenAI Service directly via the API to extract structured JSON data from the image.\n",
    "\n",
    "In this example, the payload for the Chat completion endpoint is a JSON object with the following details:\n",
    "\n",
    "### System Prompt\n",
    "\n",
    "The system prompt is the instruction to the model that prescribes the model's behavior. They allow you to constrain the model's behavior to a specific task, making it more adaptable for specific use cases, such as extracting structured JSON data from documents.\n",
    "\n",
    "In this case, it is to extract structured JSON data from the image. Here is what we have provided:\n",
    "\n",
    "**You are an AI assistant that extracts data from documents and returns them as structured JSON objects. If a value is not present, provide null. The TR4 document has multiple sections with location information, applicant information, plot diagram, and test report. Do not return as a code block. Use only information extracted from image. Do not make up any information.**\n",
    "\n",
    "> **Note:** GPT-4 Vision doesn't currently allow the `response_format` parameter to be set to `json`. To avoid the response being returned as a code block, we have included the instruction to not return as a code block. \n",
    "\n",
    "Learn more about [system prompts](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/system-message).\n",
    "\n",
    "### User Prompt\n",
    "\n",
    "The user prompt is the input to the model that provides context for the model's response. It is the input that the model uses to generate a response. \n",
    "\n",
    "In this case, it is the image of the document plus some additional text context to help the model understand the task. Here is what we have provided:\n",
    "\n",
    "**Extract the data from this Soil Investigation Technical Report. There may be multiple Boring pits and soil Test Pits in section 5 Test Report in which case an array can be created for each boring. For the Plot Diagram sections, provide a textual summary for the PlotDiagramSummaryDetails field according to the extracted measurements for an engineer to read. The plot diagram is for the address found in location information section of the document. The Plot diagram drawing will show boring points. Extract the distance for each boring if available in the image and place in the respective DistanceFromNorthBoundary, DistanceFromSouthBoundary, DistanceFromEastBoundary, or DistanceFromSouthBoundary fields. If no value then value should be null. Use the following structure, {\\\"DocumentInformation\\\", {\\\"DocumentType\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"JobNumber\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"ScanCode\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}}, \\\"LocationDetails\\\", {\\\"HouseNumber\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"StreetName\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"Borough\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"Block\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"Lot\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"BIN\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"CommunityBoardNumber\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"WorkOnFloors\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"ApartmentNumber\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}}, \\\"ApplicantInformation\\\", {\\\"LastName\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"FirstName\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"MiddleInitial\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"BusinessName\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"BusinessAddress\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"BusinessPhone\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"MobilePhone\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"Fax\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"State\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"City\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"Zip\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"Email\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"LicenseNumber\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"PEcheck\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"RAcheck\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}}, \\\"BoringDetails\\\", [{\\\"BoringNumber\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"PlotDiagramSummary\\\", \\\"PlotDiagramSummaryDetails\\\",{\\\"DistanceFromNorthBoundary\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"DistanceFromEastBoundary\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"DistanceFromSouthBoundary\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"DistanceFromWestBoundary\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}}, \\\"BoringDate\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"FeetBelowCurb\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"SoilDescription\\\", [{\\\"Depth\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"Description\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"ClassNumber\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"Remarks\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}}]}], \\\"AdditionalRemarks\\\", {\\\"Remarks\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}}}\" }**\n",
    "\n",
    "> **Note:** For the user prompt, it is ideal to provide a structure for the JSON response. Without one, the model will determine this for you and you may not get consistency across responses. \n",
    "\n",
    "This prompt ensures that the model understands the task, and the additional text context provides the model with the necessary information to extract the structured JSON data from the image. This approach would result in a response similar to the following:\n",
    "\n",
    "```json\n",
    "{  \n",
    "  \"DocumentInformation\": {  \n",
    "    \"DocumentType\": {  \n",
    "      \"value\": \"TR4 Technical Report\",  \n",
    "      \"confidence\": \"High\"  \n",
    "    },  \n",
    "    \"JobNumber\": {  \n",
    "      \"value\": \"321494037\",  \n",
    "      \"confidence\": \"High\"  \n",
    "    },  \n",
    "    \"ScanCode\": {  \n",
    "      \"value\": \"708\",  \n",
    "      \"confidence\": \"High\"  \n",
    "    }  \n",
    "  },  \n",
    "  \"LocationDetails\": {  \n",
    "    \"HouseNumber\": {  \n",
    "      \"value\": \"36-25\",  \n",
    "      \"confidence\": \"High\"  \n",
    "    },  \n",
    "    \"StreetName\": {  \n",
    "      \"value\": \"New Haven Crescent Street\",  \n",
    "      \"confidence\": \"High\"  \n",
    "    },  \n",
    "    \"Borough\": {  \n",
    "      \"value\": \"Queens\",  \n",
    "      \"confidence\": \"High\"  \n",
    "    },  \n",
    "    \"Block\": {  \n",
    "      \"value\": \"1234\",  \n",
    "      \"confidence\": \"High\"  \n",
    "    },  \n",
    "    \"Lot\": {  \n",
    "      \"value\": \"56\",  \n",
    "      \"confidence\": \"High\"  \n",
    "    },  \n",
    "    \"BIN\": {  \n",
    "      \"value\": null,  \n",
    "      \"confidence\": \"Null\"  \n",
    "    },  \n",
    "    \"CommunityBoardNumber\": {  \n",
    "      \"value\": null,  \n",
    "      \"confidence\": \"Null\"  \n",
    "    },  \n",
    "    \"WorkOnFloors\": {  \n",
    "      \"value\": \"NA/Concrete Wall\",  \n",
    "      \"confidence\": \"High\"  \n",
    "    },  \n",
    "    \"ApartmentNumber\": {  \n",
    "      \"value\": null,  \n",
    "      \"confidence\": \"Null\"  \n",
    "    }  \n",
    "  },  \n",
    "  \"ApplicantInformation\": {  \n",
    "    \"LastName\": {  \n",
    "      \"value\": \"THOMAS\",  \n",
    "      \"confidence\": \"High\"  \n",
    "    },  \n",
    "    \"FirstName\": {  \n",
    "      \"value\": \"KENNETH\",  \n",
    "      \"confidence\": \"High\"  \n",
    "    },  \n",
    "    \"MiddleInitial\": {  \n",
    "      \"value\": null,  \n",
    "      \"confidence\": \"Null\"  \n",
    "    },  \n",
    "    \"BusinessName\": {  \n",
    "      \"value\": \"INSPECTION AND TESTING SERVICES INC.\",  \n",
    "      \"confidence\": \"High\"  \n",
    "    },  \n",
    "    \"BusinessAddress\": {  \n",
    "      \"value\": \"107-13 JAMAICA AVENUE\",  \n",
    "      \"confidence\": \"High\"  \n",
    "    },  \n",
    "    \"BusinessPhone\": {  \n",
    "      \"value\": \"718-441-0600\",  \n",
    "      \"confidence\": \"High\"  \n",
    "    },  \n",
    "    \"MobilePhone\": {  \n",
    "      \"value\": \"718-555-3451\",  \n",
    "      \"confidence\": \"High\"  \n",
    "    },  \n",
    "    \"Fax\": {  \n",
    "      \"value\": null,  \n",
    "      \"confidence\": \"Null\"  \n",
    "    },  \n",
    "    \"State\": {  \n",
    "      \"value\": \"NY\",  \n",
    "      \"confidence\": \"High\"  \n",
    "    },  \n",
    "    \"City\": {  \n",
    "      \"value\": \"Richmond Hill\",  \n",
    "      \"confidence\": \"High\"  \n",
    "    },  \n",
    "    \"Zip\": {  \n",
    "      \"value\": \"11418\",  \n",
    "      \"confidence\": \"High\"  \n",
    "    },  \n",
    "    \"Email\": {  \n",
    "      \"value\": null,  \n",
    "      \"confidence\": \"Null\"  \n",
    "    },  \n",
    "    \"LicenseNumber\": {  \n",
    "      \"value\": \"10001\",  \n",
    "      \"confidence\": \"High\"  \n",
    "    },  \n",
    "    \"PEcheck\": {  \n",
    "      \"value\": \"PE\",  \n",
    "      \"confidence\": \"High\"  \n",
    "    },  \n",
    "    \"RAcheck\": {  \n",
    "      \"value\": \"RA\",  \n",
    "      \"confidence\": \"High\"  \n",
    "    }  \n",
    "  },  \n",
    "  \"BoringDetails\": [  \n",
    "    {  \n",
    "      \"BoringNumber\": {  \n",
    "        \"value\": \"B1\",  \n",
    "        \"confidence\": \"High\"  \n",
    "      },  \n",
    "      \"PlotDiagramSummary\": {  \n",
    "        \"PlotDiagramSummaryDetails\": \"The plot diagram shows one boring point located towards the eastern side of the plot, near the boundary with Crescent Street.\",  \n",
    "        \"DistanceFromNorthBoundary\": {  \n",
    "          \"value\": null,  \n",
    "          \"confidence\": \"Null\"  \n",
    "        },  \n",
    "        \"DistanceFromEastBoundary\": {  \n",
    "          \"value\": null,  \n",
    "          \"confidence\": \"Null\"  \n",
    "        },  \n",
    "        \"DistanceFromSouthBoundary\": {  \n",
    "          \"value\": null,  \n",
    "          \"confidence\": \"Null\"  \n",
    "        },  \n",
    "        \"DistanceFromWestBoundary\": {  \n",
    "          \"value\": null,  \n",
    "          \"confidence\": \"Null\"  \n",
    "        }  \n",
    "      },  \n",
    "      \"BoringDate\": {  \n",
    "        \"value\": \"01-15-15\",  \n",
    "        \"confidence\": \"High\"  \n",
    "      },  \n",
    "      \"FeetBelowCurb\": {  \n",
    "        \"value\": null,  \n",
    "        \"confidence\": \"Null\"  \n",
    "      },  \n",
    "      \"SoilDescription\": [  \n",
    "        {  \n",
    "          \"Depth\": {  \n",
    "            \"value\": \"0-2\",  \n",
    "            \"confidence\": \"High\"  \n",
    "          },  \n",
    "          \"Description\": {  \n",
    "            \"value\": \"Unconsolidated Fills Mixed with Fine Brown Sand\",  \n",
    "            \"confidence\": \"High\"  \n",
    "          },  \n",
    "          \"ClassNumber\": {  \n",
    "            \"value\": \"4\",  \n",
    "            \"confidence\": \"High\"  \n",
    "          },  \n",
    "          \"Remarks\": {  \n",
    "            \"value\": null,  \n",
    "            \"confidence\": \"Null\"  \n",
    "          }  \n",
    "        },  \n",
    "        {  \n",
    "          \"Depth\": {  \n",
    "            \"value\": \"2-5\",  \n",
    "            \"confidence\": \"High\"  \n",
    "          },  \n",
    "          \"Description\": {  \n",
    "            \"value\": \"Granular Soils (SM) Fine Brown Sand with Pieces of Boulders\",  \n",
    "            \"confidence\": \"High\"  \n",
    "          },  \n",
    "          \"ClassNumber\": {  \n",
    "            \"value\": \"3a\",  \n",
    "            \"confidence\": \"High\"  \n",
    "          },  \n",
    "          \"Remarks\": {  \n",
    "            \"value\": \"Moist\",  \n",
    "            \"confidence\": \"High\"  \n",
    "          }  \n",
    "        }  \n",
    "      ]  \n",
    "    }  \n",
    "  ],  \n",
    "  \"AdditionalRemarks\": {  \n",
    "    \"Remarks\": {  \n",
    "      \"value\": null,  \n",
    "      \"confidence\": \"Null\"  \n",
    "    }  \n",
    "  }  \n",
    "}  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var base64Image = Convert.ToBase64String(File.ReadAllBytes(pdfImageName));\n",
    "\n",
    "JsonObject jsonPayload = new JsonObject\n",
    "{\n",
    "    {\n",
    "        \"messages\", new JsonArray \n",
    "        {\n",
    "            new JsonObject\n",
    "            {\n",
    "                { \"role\", \"system\" },\n",
    "                { \"content\", \"You are an AI assistant that extracts data from documents and returns them as structured JSON objects. If a value is not present, provide null. The TR4 document has multiple sections with location information, applicant information, plot diagram, and test report. Do not return as a code block. Use only information extracted from image. Do not make up any information.\" }\n",
    "            },\n",
    "            new JsonObject\n",
    "            {\n",
    "                { \"role\", \"user\" },\n",
    "                { \"content\",\n",
    "                    new JsonArray\n",
    "                    {\n",
    "                        new JsonObject\n",
    "                        {\n",
    "                            { \"type\", \"text\" },\n",
    "                            { \"text\", \"Extract the data from this Soil Investigation Technical Report. RAcheck and PEcheck fields are boolean values. There may be multiple Boring pits and soil Test Pits in section 5 Test Report which case an array can be created for each boring. For Plot Diagram sections, summarize the diagram for the PlotDiagramSummary field according to the cardinal direction and extracted measurements.The plot diagram is for the address found in location information section of the document. The Plot diagram image will show measurements of the borings or pits and the top of the image is North, the bottom is South, The left is west and the right is east. Be detailed with the summary, but concise for an engineer to read and include the boring location address. Extract the distances labled on the diagram for each boring if available in the image. For example if a boring is 10 feet from the top, the value for the DistanceFromTop field will be 10 and if a boring is 20 feet from the right, the value for DistanceFromRight field will be 20. The same goes for DistanceFromLeft and DistanceFromBottom. Additionally, extract the street name into StreetOnTop,StreetOnBottom,StreetOnRight, and StreetOnLeft fields where the value is the name of the street if listed on a side. If no value then value should be null.There is also a corresponding field for each extracted value called confidence that indicates probability by measuring the degree of statistical certainty that the extracted result is detected correctly. The confidence value range is a percentage from 0% (low) to 99% (high) as a decimal. Confidence value should be null if value is null. Use the following structure, {\\\"DocumentInformation\\\", {\\\"DocumentType\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"JobNumber\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"ScanCode\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}}, \\\"LocationDetails\\\", {\\\"HouseNumber\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"StreetName\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"Borough\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"Block\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"Lot\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"BIN\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"CommunityBoardNumber\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"WorkOnFloors\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"ApartmentNumber\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}}, \\\"ApplicantInformation\\\", {\\\"LastName\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"FirstName\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"MiddleInitial\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"BusinessName\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"BusinessAddress\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"BusinessPhone\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"MobilePhone\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"Fax\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"State\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"City\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"Zip\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"Email\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"LicenseNumber\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"PEcheck\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"RAcheck\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}}, \\\"BoringDetails\\\", [{\\\"BoringNumber\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"PlotDiagramSummary\\\", {\\\"DistanceFromTop\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"DistanceFromRight\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"DistanceFromBottom\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"DistanceFromLeft\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}}, {\\\"StreetOnTop\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"StreetOnRight\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"StreetOnBottom\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"StreetOnLeft\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}}, \\\"BoringDate\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"FeetBelowCurb\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"SoilDescription\\\", [{\\\"Depth\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"Description\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"ClassNumber\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}, \\\"Remarks\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}}]}], \\\"AdditionalRemarks\\\", {\\\"Remarks\\\", {\\\"value\\\", \\\"\\\", \\\"confidence\\\", \\\"\\\"}}}\" }\n",
    "                        },\n",
    "                        new JsonObject\n",
    "                        {\n",
    "                            { \"type\", \"image_url\" },\n",
    "                            { \"image_url\", new JsonObject { { \"url\", $\"data:image/jpeg;base64,{base64Image}\" } } }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    { \"model\", modelDeployment },\n",
    "    { \"max_tokens\", 4096 },\n",
    "    { \"temperature\", 0 },\n",
    "    { \"top_p\", 0 },\n",
    "};\n",
    "\n",
    "string payload = JsonSerializer.Serialize(jsonPayload, new JsonSerializerOptions\n",
    "{\n",
    "    WriteIndented = true\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "string visionEndpoint = $\"{endpoint}openai/deployments/{modelDeployment}/chat/completions?api-version={apiVersion}\";\n",
    "\n",
    "using (HttpClient httpClient = new HttpClient())\n",
    "{\n",
    "    httpClient.Timeout = TimeSpan.FromSeconds(300); // Set timeout to 300 seconds\n",
    "    httpClient.BaseAddress = new Uri(visionEndpoint);\n",
    "    httpClient.DefaultRequestHeaders.Add(\"api-key\", apiKey);\n",
    "    httpClient.DefaultRequestHeaders.Accept.Add(new System.Net.Http.Headers.MediaTypeWithQualityHeaderValue(\"application/json\"));\n",
    "\n",
    "    var stringContent = new StringContent(payload, Encoding.UTF8, \"application/json\");\n",
    "\n",
    "    var response = await httpClient.PostAsync(visionEndpoint, stringContent);\n",
    "\n",
    "    if (response.IsSuccessStatusCode)\n",
    "    {\n",
    "        using (var responseStream = await response.Content.ReadAsStreamAsync())\n",
    "        {\n",
    "            // Parse the JSON response using JsonDocument\n",
    "            using (var jsonDoc = await JsonDocument.ParseAsync(responseStream))\n",
    "            {\n",
    "                // Access the message content dynamically\n",
    "                JsonElement jsonElement = jsonDoc.RootElement;\n",
    "                string messageContent = jsonElement.GetProperty(\"choices\")[0].GetProperty(\"message\").GetProperty(\"content\").GetString();\n",
    "\n",
    "                // Output the message content\n",
    "                Console.WriteLine(messageContent);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        Console.WriteLine(response);\n",
    "    }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "python"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
